use std::path::PathBuf;
use std::sync::{Arc, Barrier};
use std::thread;

use crate::async_mcts::search::SearchWorker;
use crate::async_mcts::tree::Tree;
use crate::eval_queue::{EvalQueue, EvalResult, GpuHandle};
use crate::neural_net::{load_model, nn_eval_batch};
use crate::symmetry::get_symmetries;
use anyhow::Result;
use indicatif::ProgressStyle;
use othello::othello_game::{Color, Move, OthelloGame};
use rand::distr::weighted::WeightedIndex;
use rand::prelude::*;
use rand::rng;
use rayon::ThreadPoolBuilder;
use rayon::prelude::*;
use tracing::{debug, info, info_span};
use tracing_indicatif::span_ext::IndicatifSpanExt;

/// Represents a single self-play training sample generated by MCTS.
///
/// Each sample corresponds to a game state encountered during self-play,
/// paired with:
/// - an encoded board state from the current player's perspective,
/// - a policy target derived from MCTS visit counts over all moves,
/// - a value target corresponding to the final game outcome.
///
/// Samples are later augmented via board symmetries before being used
/// for neural network training.
#[derive(Clone)]
pub struct Sample {
    pub state: [[[i32; 8]; 8]; 2], // encoded board
    pub policy: Vec<f32>,          // length 64
    pub value: f32,                // final game result
}

/// Plays a single self-play Othello game using MCTS guided by a neural network.
///
/// For each move:
/// - Runs MCTS with a shared evaluation queue and multiple tree search threads.
/// - Injects Dirichlet noise at the root for exploration.
/// - Extracts a policy target from visit counts.
/// - Selects a move using temperature-controlled sampling.
///
/// After the game ends, final outcomes are backfilled into all collected
/// samples, which are then augmented using board symmetries.
///
/// # Parameters
/// - `game_idx`: Index of the game (used for logging/debugging).
/// - `prefix`: Identifier for the current self-play run.
/// - `sims_per_move`: Number of MCTS simulations per move.
/// - `eval_queue`: Shared evaluation queue for neural network inference.
/// - `tree_threads`: Number of parallel MCTS worker threads per move.
/// - `iteration`: Training iteration number (affects exploration noise).
/// - `no_early_noise_reduction`: Disables reduced noise in early iterations.
///
/// # Returns
/// A vector of training samples generated from the game.
fn play_one_game(
    game_idx: usize,
    prefix: &str,
    sims_per_move: u32,
    eval_queue: EvalQueue,
    tree_threads: usize,
    iteration: u32,
    no_early_noise_reduction: bool,
) -> Result<Vec<Sample>> {
    debug!("Entering game {game_idx}");
    let mut game = OthelloGame::new();

    let mut game_samples: Vec<(Sample, Color)> = Vec::new();
    let mut move_number = 0u32;

    while !game.game_over() {
        let current_player = game.current_turn;
        let legal_moves = game.legal_moves(current_player);

        if legal_moves.is_empty() {
            game.mcts_play(Move::Pass, current_player).unwrap();
            move_number += 1;
            continue;
        }

        // MCTS
        let tree = Tree::new();
        let search_handle = eval_queue.search_handle();

        // First, expand the root with a single simulation
        {
            let mut init_worker = SearchWorker::new(tree.clone(), search_handle.clone());
            init_worker.simulate(&game);
            while init_worker.has_pending() {
                init_worker.poll_results();
                std::thread::yield_now();
            }
        }

        // Add Dirichlet noise to the root node for exploration
        // Use less noise in early iterations (0-2) when the network is weak/random
        // This lets MCTS concentrate visits on promising moves
        let dirichlet_epsilon = if no_early_noise_reduction {
            0.25
        } else if iteration <= 2 {
            0.15
        } else {
            0.25
        };
        tree.add_dirichlet_noise(tree.root(), 0.3, dirichlet_epsilon);

        // Run the rest of the simulations in parallel
        let num_threads = tree_threads;
        let barrier = Arc::new(Barrier::new(num_threads));

        let mut workers = Vec::new();

        for _ in 0..num_threads {
            let tree = tree.clone();
            let handle = search_handle.clone();
            let barrier = Arc::clone(&barrier);
            let root_game = game;

            workers.push(thread::spawn(move || {
                let mut worker = SearchWorker::new(tree, handle);
                barrier.wait();

                // Subtract 1 because we already did one simulation for root expansion
                let remaining_sims = sims_per_move.saturating_sub(1);
                let sims_per_thread = (remaining_sims / num_threads as u32).max(1);

                for _ in 0..sims_per_thread {
                    worker.simulate(&root_game);
                    worker.poll_results();
                }

                while worker.has_pending() {
                    worker.poll_results();
                    std::thread::yield_now();
                }
            }));
        }

        for w in workers {
            w.join().unwrap();
        }

        // Policy extraction with temperature sharpening
        // Early moves (< 10): use temp=1.0 to preserve exploration signal
        // Later moves: use argmax (T=0) for clearest learning signal
        let use_argmax = move_number >= 10;
        
        let mut policy = vec![0.0f32; 64];
        let visits = tree.child_visits(tree.root());
        
        if use_argmax {
            // T=0: put all mass on the most-visited action
            if let Some((best_action, _)) = visits.iter().max_by_key(|(_, v)| *v) {
                policy[*best_action] = 1.0;
            }
        } else {
            // T=1: use raw visit proportions
            let total_visits: u32 = visits.iter().map(|(_, v)| *v).sum::<u32>().max(1);
            for (action, count) in visits {
                policy[action] = count as f32 / total_visits as f32;
            }
        }

        // Store sample
        game_samples.push((
            Sample {
                state: game.encode(current_player),
                policy: policy.clone(),
                value: 0.0,
            },
            current_player,
        ));

        // Play move
        // Temperature annealing: use temp=1.0 for first 10 moves (exploration),
        // then temp=0.05 for remaining moves (exploitation)
        let temperature = if move_number < 10 { 1.0f32 } else { 0.05f32 };

            let mut probs: Vec<f32> = legal_moves
                .iter()
                .map(|(r, c)| {
                    let p = policy[r * 8 + c];
                    if temperature < 1.0 {
                        // Apply temperature: p^(1/T), then re-normalise
                        p.powf(1.0 / temperature)
                    } else {
                        p
                    }
                })
                .collect();

            let sum: f32 = probs.iter().sum();
            if sum <= f32::EPSILON {
                let u = 1.0 / probs.len() as f32;
                probs.iter_mut().for_each(|p| *p = u);
            } else {
                // Normalise after temperature scaling
                probs.iter_mut().for_each(|p| *p /= sum);
            }

            let dist = WeightedIndex::new(&probs)?;
            let choice = dist.sample(&mut rng());
            let (r, c) = legal_moves[choice];
            game.mcts_play(Move::Move(r, c), current_player).unwrap();
        move_number += 1;
    }

    // Backfill values
    // Note: score() returns (white_count, black_count)
    let (white_score, black_score) = game.score();
    // outcome is from Black's perspective: +1 if Black wins, -1 if White wins
    let outcome = match black_score.cmp(&white_score) {
        std::cmp::Ordering::Greater => 1.0,
        std::cmp::Ordering::Less => -1.0,
        _ => 0.0,
    };

    let samples = game_samples
        .into_iter()
        .map(|(mut s, p)| {
            s.value = if p == Color::Black { outcome } else { -outcome };
            s
        })
        .flat_map(get_symmetries)
        .collect::<Vec<_>>();

    info!(
        "Finished game {} (prefix: {}, samples {})",
        game_idx,
        prefix,
        samples.len()
    );

    Ok(samples)
}

/// Generates self-play training data by running many games in parallel.
///
/// This function:
/// - Initialises shared GPU evaluation infrastructure.
/// - Spawns a dedicated GPU worker thread for neural network inference.
/// - Runs multiple self-play games in parallel using a Rayon thread pool.
/// - Aggregates and returns all generated training samples.
///
/// Progress is reported via tracing and an indicatif progress bar.
///
/// # Parameters
/// - `prefix`: Identifier for the self-play run (used in logging).
/// - `games`: Number of self-play games to generate.
/// - `sims_per_move`: Number of MCTS simulations per move.
/// - `model`: Path to the neural network model.
/// - `game_threads`: Number of parallel self-play games to run.
/// - `tree_threads`: Number of MCTS threads per game.
/// - `iteration`: Training iteration number.
/// - `no_early_noise_reduction`: Disables reduced Dirichlet noise in early iterations.
///
/// # Returns
/// A vector containing all self-play samples from all games.
pub fn generate_self_play_data(
    prefix: &str,
    games: usize,
    sims_per_move: u32,
    model: PathBuf,
    game_threads: usize,
    tree_threads: usize,
    iteration: u32,
    no_early_noise_reduction: bool,
) -> Result<Vec<Sample>> {
    // Shared GPU infra
    let eval_queue = EvalQueue::new();
    let _gpu_thread = start_gpu_worker(eval_queue.gpu_handle(), model, 128);

    let span = info_span!("self_play");
    span.pb_set_length(games as u64);
    span.pb_set_style(
        &ProgressStyle::with_template(
            "{span_child_prefix}[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} games",
        )?
        .progress_chars("##-"),
    );
    let _guard = span.enter();

    // Parallel self-play thread pool
    let pool = ThreadPoolBuilder::new()
        .num_threads(game_threads)
        .build()?;

    // Play games in parallel
    let all_samples: Result<Vec<Sample>> = pool.install(|| {
        (0..games)
            .into_par_iter()
            .map(|game_idx| {
                let res = play_one_game(
                    game_idx,
                    prefix,
                    sims_per_move,
                    eval_queue.clone(),
                    tree_threads,
                    iteration,
                    no_early_noise_reduction,
                );
                // Increment the progress bar
                span.pb_inc(1);
                res
            })
            // Parallelly collect and flatten all game samples into a single Result<Vec<Sample>>.
            .try_reduce(Vec::new, |mut acc, mut samples| {
                acc.append(&mut samples);
                Ok(acc)
            })
    });

    all_samples
}

/// Spawns a dedicated GPU worker thread for batched neural network evaluation.
///
/// The worker:
/// - Loads the neural network model from disk.
/// - Continuously pulls batches of evaluation requests from the GPU handle.
/// - Runs batched inference on the neural network.
/// - Pushes policy and value results back to the evaluation queue.
///
/// This thread runs indefinitely and is intended to service all MCTS
/// workers via a shared queue.
///
/// # Parameters
/// - `gpu`: Handle used to receive evaluation requests and return results.
/// - `model_path`: Path to the serialized neural network model.
/// - `max_batch_size`: Maximum number of requests to evaluate per batch.
///
/// # Returns
/// A join handle for the spawned GPU worker thread.
pub fn start_gpu_worker(
    gpu: GpuHandle,
    model_path: PathBuf,
    max_batch_size: usize,
) -> thread::JoinHandle<()> {
    thread::spawn(move || {
        debug!(
            "GPU worker: starting, attempting to load model from {:?}",
            model_path
        );
        let mut model = load_model(model_path.to_str().unwrap()).expect("failed to load model");
        debug!("GPU worker: model loaded successfully");

        loop {
            // Pop a batch of requests
            let batch = gpu.pop_batch(max_batch_size);

            if batch.is_empty() {
                // debug!("No batch available, yielding...");
                std::thread::yield_now();
                continue;
            }
            debug!("GPU worker: processing batch of {} requests", batch.len());

            // Extract states and IDs from requests
            let states: Vec<Vec<f32>> = batch.iter().map(|req| req.state.clone()).collect();
            let ids: Vec<u64> = batch.iter().map(|req| req.id).collect();

            // Evaluate the batch with the neural network
            let evals = match nn_eval_batch(&mut model, &states) {
                Ok(evals) => {
                    debug!("GPU worker: batch evaluation succeeded");
                    evals
                }
                Err(e) => {
                    debug!("GPU worker: batch evaluation failed: {:?}", e);
                    return;
                }
            };

            // Convert NN output to EvalResults
            let mut results = Vec::with_capacity(evals.len());
            for ((policy_map, value), id) in evals.into_iter().zip(ids) {
                // Flatten the policy map to (action_index, probability) pairs
                let policy_flat: Vec<(usize, f32)> = policy_map
                    .into_iter()
                    .map(|((r, c), p)| (r * 8 + c, p))
                    .collect();

                results.push(EvalResult {
                    id,
                    policy: policy_flat,
                    value,
                });
            }

            // Push results back to the queue
            debug!("GPU worker: pushing {} results back", results.len());
            gpu.push_results(results);
        }
    })
}
